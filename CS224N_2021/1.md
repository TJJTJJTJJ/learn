# 2020-09-14

开始学习CS224N，之前看了一部分，但是已经忘记看到哪里了，索性重新看一遍，并且记一些笔记
b站：https://www.bilibili.com/video/BV18Y411p79k?p=1&vd_source=5dfb70630ca8dca05b6a4596cf8fc9dc
cs224n: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/index.html

# Lecture1: Introduction and word vectors

PPT: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/slides/cs224n-2021-lecture01-wordvecs1.pdf

1. WordNet
   WordNet: A Lexical Database for English：大型的英文词汇数据库、同义词集合。
   https://zhuanlan.zhihu.com/p/366370332
   缺点：

- 消歧问题
- 不能计算词之间的相似性：fantastic and great，不是同义词，但是都表示类似的含义
- 静态的词典，动态增加的成本很高

2. 分布式表示、连续的词向量

- 传统的机器学习：把单词看成离散的one-hot编码，不能很好地表示词与词之间的关系和相似性
  VS
- 现在的深度学习把词表示成连续的特征向量，用于表示词之间的相似性。

如何表示单词的向量：分布式语义 distributional semantics：由周围的单词表示。贯穿统计学和深度学习。
单词的向量 word vector：在相似的上下文中出现的单词之间的相似性高。

3. word2vec

https://zhuanlan.zhihu.com/p/114538417
https://zhuanlan.zhihu.com/p/26306795

Skip-gram: 中心词预测上下文
CBOW：上下文预测中心词
隐藏层没有激活函数
一般使用输入向量
Hierarchical Softmax： 待学
Negative Sampling： 待学
问题：

- 一个词有多种含义时，依然用同一个词向量表示

# Lecture2：Neural Claassifiers

https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/slides/cs224n-2021-lecture02-wordvecs2.pdf

1. word2vec

- 优点:
  - 能表示词之间的相似性
  - 能表示词之间的相加相减性
- 缺点：
  - 不能区分context和center的远近

1.1 negative sampling

将交叉熵损失函数变成二元分类函数，判断词对是不是属于相邻词，减少计算量
paper: Distributed Representations of Words and Phrases and their
Compositionality
$J = -\log(\sigma(u_o^T*v_c)) - \sum_{k=取样的k个负样本}{\log(\sigma(u_k^T*v_c))}$

1.2 word2vec vs 共现矩阵+SVD分解

count_based:

- 用于计算单词相似性
- 对经常出现的词给了不恰当的重要性

word2vec:

- 能用于下游任务
- 除了相似性，还能捕捉到其他模式特征

1.3 word2vec vs glove

glove 公式推导由来
https://zhuanlan.zhihu.com/p/58389508
虽然觉得其中有些地方的说服力不是很强，但是也能说通
44：51

6. 如何评估词向量

固有的：

- 在具体的子任务上
- 快

外在的：

真实任务

慢

# Lecture3: Backprop

https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/slides/cs224n-2021-lecture03-neuralnets.pdf

# Lecture4: Dependency Parsing

https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/slides/cs224n-2021-lecture04-dep-parsing.pdf

20：55

dependency parsing : 句法结构解析

对于 **句法结构(syntactic structure)** 分析，主要有两种方式：Constituency Parsing与Dependency Parsing。

---

**Constituency Parsing**

固定的语法规则 （可以简单理解成正则表达式） -- 上下文无关语法

先分析每个单词的词性，然后根据语法规则，从少到多，不断组合成短语

---

Dependency Parsing

一句话中单词之间的依存关系

语法结构解析很容易有歧义

方法有多种：

- transition-based parsing or deterministic dependency parsing
- graph-based parser

![1667229106197](image/1/1667229106197.png)

参考资料：

https://zhuanlan.zhihu.com/p/66268929

https://blog.csdn.net/qq_27586341/article/details/110001823

---

dependency grammerand dependency structure

tree bank

# Lecture5: Language Models and RNNs

---

transition-based parsing or deterministic dependency parsing

![1668092218993](image/1/1668092218993.png)

https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/slides/cs224n-2021-lecture05-rnnlm.pdf

Neural Dependency Parser Model: Chen and Manning (2014)：性能高，速度快

![1668091425910](image/1/1668091425910.png)

---

graph-based parser

![1668092265468](image/1/1668092265468.png)

---

神经网络：

L2正则

dropout

```python
import numpy as np

def dropout(x, level):
    # p表示drop的概率
    if level < 0 or level >= 1:  # p，必须在0~1之间
        raise Exception('Dropout p must be in interval [0, 1]')
    retain_prob = 1. - level
    # 通过binomial函数，生成与数据x一样维数的向量。
    # size参数是就是有多少个数据，比如全连接某一中间层的输入数据
    # p = retain_prob: 输入数据保留的概率
    # 即将生成一个0、1分布的向量
    sample = np.random.binomial(n=1, p=retain_prob, size=x.shape)
    # 0、1与数据x相乘，我们就可以屏蔽某些神经元，让它们的值变为0，1则不影响
    x *= sample
    # 保持dropOut前后数据期望的一致性
    x /= retain_prob
    return x

if __name__ == '__main__':
    x = np.asarray([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float32)
    print("输入：", x)
    out = dropout(x, 0.5)
    print("输出：", out)
```

非线性函数：sigmoid、tanh、hard tanh、relu

模型初始化：

- 线性层：[xavier:](https://zjuturtle.com/2018/05/10/xavier-init/#:~:text=Xavier%20%E6%96%B9%E6%B3%95%E6%8F%90%E4%BE%9B%E4%BA%86%E4%B8%80%E4%B8%AA%E5%90%88%E7%90%86%E7%9A%84%E6%96%B9%E5%BC%8F%E6%9D%A5%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8D%E3%80%82%20%E7%AE%80%E5%8D%95%E6%9D%A5%E8%AF%B4%EF%BC%8C%E5%B0%B1%E6%98%AF%E5%B0%86%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E5%85%83%E7%9A%84%E5%88%9D%E5%A7%8B%E5%80%BC%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%BA%E5%9D%87%E5%80%BC%E4%B8%BA0%EF%BC%8C%E6%96%B9%E5%B7%AE%E4%B8%BA%20V%20a%20r%20%28w%20i%29,i%20n%20%E7%9A%84%E9%9A%8F%E6%9C%BA%E5%88%86%E5%B8%83%EF%BC%88%E9%AB%98%E6%96%AF%E6%88%96%E8%80%85%E5%9D%87%E5%8C%80%E5%88%86%E5%B8%83%EF%BC%89%E3%80%82%20%E5%85%B6%E4%B8%AD%20n%20i%20n%20%E6%98%AF%E8%AF%A5%E7%A5%9E%E7%BB%8F%E5%85%83%E7%9A%84%E8%BE%93%E5%85%A5%E6%95%B0%E7%9B%AE%E3%80%82)
- conv: kaiming 初始化

优化器： SGD，Adam

学习率

---

Language Model：给定前面的词，预测下一个词是什么

![1668261301564](image/1/1668261301564.png)

---

n-gram language model

- 稀疏性1：分子没有出现过：添加一个极小值
- 稀疏性2：分母没有出现过：用n-1 gram代替
- 存储空间大：需要存储所有的n-gram的数量
- ![1668263001273](image/1/1668263001273.png)

---

neural language model

fixed-window neural language model

![1668329625778](image/1/1668329625778.png)

Recurrent Language Model

![1668330354043](image/1/1668330354043.png)

---

# Lecture6: Simple and LSTM RNNs

https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/slides/cs224n-2021-lecture06-fancy-rnn.pdf

RNN -- Language Model

---

Training an RNN Language Model

![1670143139377](image/1/1670143139377.png)

![1668332214911](image/1/1668332214911.png)

teacher forcing: 每次将真值作为输入，而不是上一个预测值作为输入。

优点是训练收敛速度快，缺点是会有 exposure bias，因为训练和预测分布不一致，或者理解成预测时，某个词预测错误后，后面就会一直跟着错。

解决方案有 scheduled sampling（即随机一部分用真值，一部分用上一个预测值），beam searching

https://blog.csdn.net/qq_30219017/article/details/89090690

https://zhuanlan.zhihu.com/p/93030328

https://zhuanlan.zhihu.com/p/128003099

---

语言模型：

loss：交叉熵损失

指标：

perplexity：困惑度，等价于交叉熵，值越小，说明模型效果越好

困惑度，比如53，可以理解成平均下一个词预测正确的概率是1/53.

- https://www.zhihu.com/question/58482430

![1668527057884](image/1/1668527057884.png=40*30)

![1668527069458](image/1/1668527069458.png)

![1668527386545](image/1/1668527386545.png)

---

RNN 的使用场景：

* 序列标注 sequence tagging: 词性标注，NER
* 文本分类
* 问答
* 机器翻译
* 语音识别：conditional language model

---

梯度消失和梯度爆炸问题：

梯度消失和梯度爆炸本质上是一样的，均因为网络层数太深而引发的梯度反向传播中的连乘效应。

梯度消失

梯度消失的原因：链式法则累乘，值都小于1的情况下，乘积结果会趋近于0.

梯度消失造成的问题：RNN只适合对短距离进行建模，但是对于长距离效果一般

梯度爆炸：梯度更新会变成nan

解决方法：梯度剪枝：缩小梯度范数

![1670141492062](image/1/1670141492062.png)

参考链接：https://zhuanlan.zhihu.com/p/483651927

---

LSTM：为了解决梯度消失问题，1999年提出

![1670142998328](image/1/1670142998328.png)

LSTM发明于2000年，但实际成为主导在2013-2019年，之后transformers成为了主导。

解决梯度消失：是因为反向传播的公式中有求和。当$f_t$趋近于1时，偏导至少为1，当$f_t$等于0时，本身反向传播也不应该传播到低层模型。（PS：看了网上一些关于LSTM对梯度消失的解释，是大相径庭的，我比较认可数学公式的反向推导）。或者说：更重要的是，能将底层模型的输出结果直接带到高层，这样反向传播的误差也能直接传到低层，highway，而不需要经过中间层

https://blog.csdn.net/z2876563/article/details/113744914

$$
\frac{\partial L}{\partial W} = \frac{\partial L}{\partial y} * \frac{\partial y}{\partial h^t} * \frac{\partial h^t}{\partial c^t} * \frac{\partial c^t}{\partial c^{t-1}} * \frac{\partial c^{t-1}}{\partial c^{t-2}}...*\frac{\partial c^k}{\partial W^k}
$$

![1670147424531](image/1/1670147424531.png)

---

双向RNN

![1670148927589](image/1/1670148927589.png)

Multi-Layer RNN

![1670167889558](image/1/1670167889558.png)

# Lecture7: Translation, Seq2Seq, Attention

pdf: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/slides/cs224n-2021-lecture07-nmt.pdf

---

Language Model -- RNN -- 给定前面的词，预测下一个词

Translation Model -- Seq2Seq -- Attention -- 给定一种语言的一句话，翻译成另一种语言的一句话

---

## SMT：statistical machine learning

![1670165904550](image/1/1670165904550.png)

![1670165983373](image/1/1670165983373.png)

![1670166159080](image/1/1670166159080.png)

## Neural machine translation

neural machine translation model 又称为 sequence-to-sequence model

2014年才开始有 NMT

![1670166729896](image/1/1670166729896.png)

encoder RNN --> 生成 source sentence 的 encoding

decoder RNN --> Language Model conditioned on encoding

![1670167290757](image/1/1670167290757.png)

---

训练 NMT

![1670169827786](image/1/1670169827786.png)

---

预测NMT：beam search decoding vs greedy decoding

流程

![1670170113685](image/1/1670170113685.png)

停止机制

![1670173096248](image/1/1670173096248.png)

 对 translation 的评价指标 BLUE

https://blog.csdn.net/guolindonggld/article/details/56966200

![1671373740565](image/1/1671373740565.png)

---

## Attention

attention的原因：NMT的decoder的输入只有encoder的最后一个hidden state，是信息传递的瓶颈。

![1671375314446](image/1/1671375314446.png)

attention的做法：

![1671375158798](image/1/1671375158798.png)

![1671375188080](image/1/1671375188080.png)

attention的公式：

![1671375419003](image/1/1671375419003.png)

# Lecture8: Final project practical Tips

https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/slides/cs224n-2021-lecture08-final-project.pdf

---

attention 三步曲

![1671722363115](image/1/1671722363115.png)

attention变体

![1671722391603](image/1/1671722391603.png)

其中, reduced rank multiplicative attention 是 transformer 中使用的attention

# Lecture9: Self-Attention and Transformers

https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/slides/cs224n-2021-lecture09-transformers.pdf

## Attention Model

RNN 模型的缺点

1. RNN 模型对距离线性建模，或者叫长期依赖问题：距离越远的两个单词，互相之间的影响性越小
2. RNN 模型无法并行计算：必须先计算前一个状态，才能计算当前状态

Attention的优点：

1. attention 可以对远距离建模
2. attention在时间/距离维度可以并行计算，在深度上无法并行

Attention 和 fully connection 的区别

1. attention 是动态的，fully connection 是静态的

---

Attention 的问题一：sequence order --> position embedding

1. Sinusoidal position representations 正弦位置编码
   1. 优点：
      1. 绝对位置没有那么重要，
      2. 周期性，可以用于更长的位置
   2. 缺点：
      1. 不可学习
2. Learned absolute position representations 绝对位置编码
   1. 优点：
      1. 更灵活，参数可学习
   2. 缺点：
      1. 训练后位置长度固定，不能外延到更长的长度

---

Attention 的问题二：没有非线性 -->feedforward network

![1673187076880](image/1/1673187076880.png)

---

Attention 的问题三：不能看未来 --> mask

 ![1673187283498](image/1/1673187283498.png)

---

## Transformer Model

Transformer Model 需要涉及到的问题：

1. key-query-value attention：k, q, v 向量怎么得到
2. multi-head attention
3. tricks to help train:
   1. residual connection
   2. layer normalization
   3. scaling the dot product


03:35
